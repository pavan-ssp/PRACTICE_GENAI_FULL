{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f36f8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46094636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "# Get the value of the environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "187f6a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECTS\"] = os.getenv(\"LANGCHAIN_PROJECTS\")\n",
    "\n",
    "os.environ[\"LANCHAIN_TRACING_V2\"]= \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfdec2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_groq import ChatGroq \n",
    "#from langchain_googleapi import ChatGoogleAI\n",
    "#from langchain_huggingface import ChatHuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a34d9587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x0000022706850790> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000022706D59790> root_client=<openai.OpenAI object at 0x000002270689B650> root_async_client=<openai.AsyncOpenAI object at 0x0000022706809150> model_name='o1-mini' temperature=1.0 model_kwargs={} openai_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm=ChatOpenAI(model=\"o1-mini\")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a72c12c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Satya! It's great to meet you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "result=llm.invoke(\"hi i am satya\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24df69db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Nice to meet you, Satya! What brings you here today?' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 17, 'total_tokens': 32, 'completion_time': 0.018331795, 'prompt_time': 0.003137908, 'queue_time': 0.268417992, 'total_time': 0.021469703}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_8dc6ecaf8e', 'finish_reason': 'stop', 'logprobs': None} id='run--6bbe5a11-c866-4101-8f5c-539751188d36-0' usage_metadata={'input_tokens': 17, 'output_tokens': 15, 'total_tokens': 32}\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model = ChatGroq(model=\"llama3-8b-8192\")  # mistral-saba-24b is not the right ID\n",
    "response = model.invoke(\"Hi, my name is satya\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c4bf013d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Prompt Engineering\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer. Provide me answer based on the question\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f877337e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001FFCC6DDB90>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001FFCC6DF4D0>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model=ChatGroq(model=\"gemma2-9b-it\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c6247341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001FFCC6DDB90>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001FFCC6DF4D0>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### chaining\n",
    "chain=prompt|model\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1ec5fe72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI engineer, I'm happy to tell you about Langsmith!\n",
      "\n",
      "Langsmith is an open-source platform developed by the **AI21 Labs** team. It's designed to simplify the process of fine-tuning and deploying large language models (LLMs). Think of it as a toolbox specifically for working with LLMs.\n",
      "\n",
      "Here are some key things to know about Langsmith:\n",
      "\n",
      "**1. User-Friendly Interface:** Langsmith boasts a web-based interface that makes it accessible to both seasoned AI professionals and beginners. You don't need extensive coding knowledge to experiment with and refine LLMs.\n",
      "\n",
      "**2. Streamlined Fine-Tuning:**\n",
      "\n",
      "Langsmith provides an intuitive workflow for fine-tuning pre-trained LLMs on your own datasets. This allows you to customize the model's behavior and improve its performance for specific tasks.\n",
      "\n",
      "**3. Comprehensive Evaluation Tools:**\n",
      "\n",
      "It offers built-in tools to evaluate the performance of your fine-tuned models, helping you measure their accuracy, fluency, and other relevant metrics.\n",
      "\n",
      "**4. Deployment Flexibility:**\n",
      "\n",
      "Langsmith empowers you to deploy your fine-tuned models in various ways, including as standalone applications, APIs, or integrated into existing workflows.\n",
      "\n",
      "**5. Strong Community Support:**\n",
      "\n",
      "As an open-source project, Langsmith benefits from a vibrant community of developers and users who contribute to its growth and provide support.\n",
      "\n",
      "**In essence, Langsmith aims to democratize access to LLM technology by making it easier for everyone to leverage the power of these models.**\n",
      "\n",
      "If you're interested in exploring Langsmith further, I recommend visiting their official website and documentation. They provide detailed tutorials and examples to get you started.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response=chain.invoke({\"input\":\"Can you tell me something about Langsmith\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c91e4483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's dive into Langsmith. \n",
      "\n",
      "**Langsmith: Your Personalized AI Development Playground**\n",
      "\n",
      "Langsmith is an open-source platform designed to simplify and streamline the process of building, experimenting with, and deploying AI applications. Think of it as a comprehensive toolkit for anyone interested in harnessing the power of large language models (LLMs) without getting bogged down in complex technicalities.\n",
      "\n",
      "**Key Features:**\n",
      "\n",
      "* **Intuitive Interface:** Langsmith boasts a user-friendly web interface that makes it accessible to both seasoned developers and beginners.  It's built with simplicity and ease of use in mind.\n",
      "* **Modular Design:** The platform is structured around modular components, allowing you to easily integrate different LLMs, tools, and datasets into your workflows.  You have the flexibility to customize your setup precisely to your needs.\n",
      "* **Experimentation Focus:** Langsmith excels at fostering experimentation. It provides tools for rapidly prototyping AI applications, testing different model architectures and parameters, and quickly iterating on your ideas.\n",
      "* **Deployment Flexibility:** Once you've developed your AI solution, Langsmith offers various deployment options, including hosting it locally, on a cloud platform, or even integrating it into existing applications.\n",
      "\n",
      "**Who Benefits from Langsmith?**\n",
      "\n",
      "* **Researchers:** Explore new LLM capabilities, test hypotheses, and develop novel AI algorithms.\n",
      "* **Developers:** Build AI-powered applications faster and more efficiently, focusing on the core logic rather than infrastructure.\n",
      "* **Educators:**  Use Langsmith as a hands-on learning tool to teach students about LLMs and AI development.\n",
      "* **Anyone with an Idea:**  Even if you have limited coding experience, Langsmith provides a low-barrier entry point to experimenting with the potential of AI.\n",
      "\n",
      "**Getting Started:**\n",
      "\n",
      "To explore Langsmith, head over to the official website and documentation. They offer comprehensive guides, tutorials, and examples to help you get up and running quickly.\n",
      "\n",
      "**The Future of Langsmith:**\n",
      "\n",
      "Langsmith is an actively developed project with a vibrant community.  Expect to see continued enhancements, new features, and broader support for various LLMs in the future.  \n",
      "\n",
      "Let me know if you have any more specific questions about Langsmith. I'm here to help!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### OutputParser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser=StrOutputParser()\n",
    "\n",
    "chain=prompt|model|output_parser\n",
    "\n",
    "response=chain.invoke({\"input\":\"Can you tell me about Langsmith\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7022e4bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Return a JSON object.'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "output_parser=JsonOutputParser()\n",
    "output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6992dbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### OutputParser\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "output_parser=JsonOutputParser()\n",
    "\n",
    "prompt=PromptTemplate(\n",
    "    template=\"Answer the user query \\n {format_instruction}\\n {query}\\n \",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instruction\":output_parser.get_format_instructions()},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "df6c0f8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['query'], input_types={}, partial_variables={'format_instruction': 'Return a JSON object.'}, template='Answer the user query \\n {format_instruction}\\n {query}\\n ')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ee1e2340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Langsmith', 'description': 'Langsmith is an open-source framework for building and deploying custom language models.', 'key_features': ['Modular and extensible design', 'Support for various model architectures', 'Easy integration with existing tools and workflows', 'Fine-tuning capabilities for domain-specific tasks', 'Community-driven development and support'], 'benefits': ['Cost-effective development of custom language models', 'Increased customization and control over model behavior', 'Improved performance for specific use cases', 'Access to a growing ecosystem of tools and resources'], 'website': 'https://github.com/facebookresearch/langs'}\n"
     ]
    }
   ],
   "source": [
    "chain=prompt|model|output_parser\n",
    "response=chain.invoke({\"query\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c89b07d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer.Provide the response in json.Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Prompt Engineering\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer.Provide the response in json.Provide me answer based on the question\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3c547cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'Langsmith is an open-source platform designed to simplify the development and deployment of large language models (LLMs).  \\n\\nHere are some key features and aspects of Langsmith:\\n\\n* **Ease of Use:** Langsmith aims to make working with LLMs more accessible to developers and researchers who may not have extensive experience in machine learning or deep learning.\\n\\n* **Model Management:** It provides tools for managing different LLM models, including popular ones like Llama, Vicuna, and GPT-Neo. Users can easily switch between models or experiment with new ones.\\n* **Fine-Tuning:** Langsmith simplifies the process of fine-tuning existing LLMs on specific tasks or datasets. This allows users to customize models for their particular needs.\\n* **Deployment:**  The platform offers options for deploying fine-tuned models as APIs or web applications, making it easier to integrate LLMs into various projects.\\n* **Community-Driven:** Langsmith is built and maintained by a community of developers and contributors, fostering collaboration and innovation in the LLM space.\\n\\n* **Open-Source Nature:** Being open-source, Langsmith allows for transparency, customization, and community improvements.\\n\\n**Key Benefits:**\\n\\n* **Lower Barrier to Entry:** Makes LLMs more approachable for a wider range of users.\\n* **Increased Productivity:** Streamlines the development workflow for LLM applications.\\n* **Flexibility and Customization:** Enables users to tailor LLMs to their specific requirements.\\n* **Cost-Effectiveness:**  Leveraging open-source models and tools can reduce development costs.\\n\\n**Overall, Langsmith is a valuable resource for anyone interested in exploring and utilizing the power of large language models in a more accessible and efficient manner.**'}\n"
     ]
    }
   ],
   "source": [
    "chain=prompt|model|output_parser\n",
    "response=chain.invoke({\"input\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "981630b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer.<response><answer>Your answer here</answer></response>.Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### OutputParser\n",
    "from langchain_core.output_parsers import XMLOutputParser\n",
    "output_parser=XMLOutputParser()\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer.<response><answer>Your answer here</answer></response>.Provide me answer based on the question\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e3184bd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['query'], input_types={}, partial_variables={'format_instruction': 'The output should be formatted as a XML file.\\n1. Output should conform to the tags below.\\n2. If tags are not given, make them on your own.\\n3. Remember to always open and close all the tags.\\n\\nAs an example, for the tags [\"foo\", \"bar\", \"baz\"]:\\n1. String \"<foo>\\n   <bar>\\n      <baz></baz>\\n   </bar>\\n</foo>\" is a well-formatted instance of the schema.\\n2. String \"<foo>\\n   <bar>\\n   </foo>\" is a badly-formatted instance.\\n3. String \"<foo>\\n   <tag>\\n   </tag>\\n</foo>\" is a badly-formatted instance.\\n\\nHere are the output tags:\\n```\\nNone\\n```'}, template='Answer the user query \\n {format_instruction}\\n {query}\\n ')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### OutputParser\n",
    "from langchain_core.output_parsers import XMLOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "output_parser=XMLOutputParser()\n",
    "\n",
    "prompt=PromptTemplate(\n",
    "    template=\"Answer the user query \\n {format_instruction}\\n {query}\\n \",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instruction\":output_parser.get_format_instructions()},\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e10fa39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='```xml\\n<response>\\n  <name>Langsmith</name>\\n  <description>Langsmith is an open-source platform for building and deploying language models.</description>\\n  <features>\\n    <feature>Modular design</feature>\\n    <feature>Fine-tuning capabilities</feature>\\n    <feature>Community-driven development</feature>\\n  </features>\\n</response>\\n``` \\n' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 92, 'prompt_tokens': 195, 'total_tokens': 287, 'completion_time': 0.167272727, 'prompt_time': 0.012588126, 'queue_time': 0.298912654, 'total_time': 0.179860853}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run--0f040533-5d75-4fd2-9a7e-a260dd1455f2-0' usage_metadata={'input_tokens': 195, 'output_tokens': 92, 'total_tokens': 287}\n"
     ]
    }
   ],
   "source": [
    "chain=prompt|model\n",
    "response=chain.invoke({\"query\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ae96e08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='<response><answer>LangChain is an open-source framework designed to simplify the development of applications powered by large language models (LLMs). It provides tools and components to help you build, chain, and manage LLM-based workflows, making it easier to integrate these powerful models into your projects.</answer></response> \\n' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 69, 'prompt_tokens': 39, 'total_tokens': 108, 'completion_time': 0.125454545, 'prompt_time': 0.002432115, 'queue_time': 0.260947842, 'total_time': 0.12788666}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run--5beac50a-ccf8-433b-819e-519c2f90343f-0' usage_metadata={'input_tokens': 39, 'output_tokens': 69, 'total_tokens': 108}\n"
     ]
    }
   ],
   "source": [
    "##output parser\n",
    "#from langchain_core.output_parsers import XMLOutputParser\n",
    "from langchain.output_parsers.xml import XMLOutputParser\n",
    "\n",
    "# XML Output Parser\n",
    "output_parser = XMLOutputParser()\n",
    "\n",
    "# Prompt that instructs the model to return XML\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Respond in this XML format: <response><answer>Your answer here</answer></response>\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Build the chain\n",
    "chain = prompt | model\n",
    "\n",
    "# Run the chain\n",
    "#response = chain.invoke({\"input\": \"What is LangChain?\"})\n",
    "\n",
    "raw_output =chain.invoke({\"input\": \"What is LangChain?\"})\n",
    "\n",
    "# Print result\n",
    "print(raw_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7e127b68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'setup': \"Why couldn't the bicycle find its way home?\",\n",
       " 'punchline': 'Because it lost its bearings!'}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## With Pydantic\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "model = ChatOpenAI(temperature=0.7)\n",
    "\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "\n",
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "joke_query = \"Tell me a joke.\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = JsonOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "207a7dd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'joke': \"Why couldn't the bicycle find its way home? Because it lost its bearings!\"}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Without Pydantic\n",
    "joke_query = \"Tell me a joke .\"\n",
    "\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "72291c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<movie>Big</movie>\n",
      "<movie>Saving Private Ryan</movie>\n",
      "<movie>Forrest Gump</movie>\n",
      "<movie>Cast Away</movie>\n",
      "<movie>Toy Story (franchise)</movie>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_core.output_parsers import XMLOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "actor_query = \"Generate the shortened filmography for Tom Hanks.\"\n",
    "\n",
    "output = model.invoke(\n",
    "    f\"\"\"{actor_query}\n",
    "Please enclose the movies in <movie></movie> tags\"\"\"\n",
    ")\n",
    "\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a3f378f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup='Why did the scarecrow win an award?', punchline='Because he was outstanding in his field!')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers import YamlOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "\n",
    "model = ChatOpenAI(temperature=0.5)\n",
    "\n",
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "joke_query = \"Tell me a joke.\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = YamlOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccf55bd",
   "metadata": {},
   "source": [
    "### Assisgment:\n",
    "Create a simple assistant that uses any LLM and should be pydantic, when we ask about any product it should give you two information product Name, product details tentative price in USD (integer). use chat Prompt Template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9e411a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Raw Response:\n",
      " name: PlayStation 5\n",
      "details: The PlayStation 5 is a next-generation gaming console developed by Sony Interactive Entertainment, featuring a fast SSD, ray tracing capabilities, and a wide range of exclusive games.\n",
      "price: 399\n",
      "\n",
      "Parsed Result:\n",
      " name='PlayStation 5' details='The PlayStation 5 is a next-generation gaming console developed by Sony Interactive Entertainment, featuring a fast SSD, ray tracing capabilities, and a wide range of exclusive games.' price=399\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq  # Or use from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Setup LLM (Groq or OpenAI)\n",
    "llm = ChatGroq(model=\"llama3-8b-8192\")  # replace with ChatOpenAI(model=\"gpt-3.5-turbo\") if needed\n",
    "\n",
    "# Define Pydantic schema\n",
    "class ProductInfo(BaseModel):\n",
    "    name: str = Field(..., description=\"Name of the product\")\n",
    "    details: str = Field(..., description=\"Short description of the product\")\n",
    "    price: int = Field(..., description=\"Tentative price in USD\")\n",
    "\n",
    "# Create prompt template\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a helpful assistant. When given a product name, provide the following:\n",
    "    - Product name\n",
    "    - A short product description\n",
    "    - A tentative price in USD (as an integer only)\n",
    "\n",
    "    Product: {product_name}\n",
    "    \n",
    "    Format the answer strictly as:\n",
    "    name: <product name>\n",
    "    details: <short description>\n",
    "    price: <integer price>\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Ask user\n",
    "user_input = \"PlayStation 5\"\n",
    "formatted_prompt = prompt.format_messages(product_name=user_input)\n",
    "\n",
    "# Get LLM response\n",
    "response = llm.invoke(formatted_prompt)\n",
    "print(\"\\nRaw Response:\\n\", response.content)\n",
    "\n",
    "# Parse with Pydantic\n",
    "def parse_response(text: str) -> ProductInfo:\n",
    "    lines = text.strip().split('\\n')\n",
    "    data = {}\n",
    "    for line in lines:\n",
    "        if ':' in line:\n",
    "            key, val = line.split(':', 1)\n",
    "            data[key.strip()] = val.strip()\n",
    "    return ProductInfo(**data)\n",
    "\n",
    "product_info = parse_response(response.content)\n",
    "\n",
    "# ✅ Final Output\n",
    "print(\"\\nParsed Result:\\n\", product_info)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practiceenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
