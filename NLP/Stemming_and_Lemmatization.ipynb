{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ba6790a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed2adaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee96e446",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph =\"\"\" In Natural Language Processing (NLP), \n",
    "               tokenization is the foundational step where raw text is split into smaller units called tokens. \n",
    "               These tokens can be words, subwords, or even characters, depending on the application. \n",
    "               Tokenization helps in understanding and processing text by breaking it down into manageable pieces.\n",
    "               After tokenization, text often goes through stemming,\n",
    "               which involves reducing words to their base or root form by removing suffixes. \n",
    "               For example, words like “playing”, “played”, and “plays” are reduced to “play”. \n",
    "               However, stemming is a rule-based and sometimes crude process, often leading to non-real words like “comput” from “computing”.\n",
    "               To overcome this, lemmatization is used, which is a more sophisticated technique \n",
    "               that transforms words to their dictionary form (lemma), taking into account the context and parts of speech. \n",
    "               For example, “running” becomes “run”, and “better” becomes “good”. \n",
    "               Lemmatization provides more meaningful results compared to stemming, although it is computationally more intensive. \n",
    "               Together, these techniques are crucial in cleaning and normalizing text for downstream NLP tasks \n",
    "               like classification, sentiment analysis, and information retrieval.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "320f32e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences: [' In Natural Language Processing (NLP), \\n               tokenization is the foundational step where raw text is split into smaller units called tokens.', 'These tokens can be words, subwords, or even characters, depending on the application.', 'Tokenization helps in understanding and processing text by breaking it down into manageable pieces.', 'After tokenization, text often goes through stemming,\\n               which involves reducing words to their base or root form by removing suffixes.', 'For example, words like “playing”, “played”, and “plays” are reduced to “play”.', 'However, stemming is a rule-based and sometimes crude process, often leading to non-real words like “comput” from “computing”.', 'To overcome this, lemmatization is used, which is a more sophisticated technique \\n               that transforms words to their dictionary form (lemma), taking into account the context and parts of speech.', 'For example, “running” becomes “run”, and “better” becomes “good”.', 'Lemmatization provides more meaningful results compared to stemming, although it is computationally more intensive.', 'Together, these techniques are crucial in cleaning and normalizing text for downstream NLP tasks \\n               like classification, sentiment analysis, and information retrieval.']\n",
      "Words: ['In', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', ',', 'tokenization', 'is', 'the', 'foundational', 'step', 'where', 'raw', 'text', 'is', 'split', 'into', 'smaller', 'units', 'called', 'tokens', '.', 'These', 'tokens', 'can', 'be', 'words', ',', 'subwords', ',', 'or', 'even', 'characters', ',', 'depending', 'on', 'the', 'application', '.', 'Tokenization', 'helps', 'in', 'understanding', 'and', 'processing', 'text', 'by', 'breaking', 'it', 'down', 'into', 'manageable', 'pieces', '.', 'After', 'tokenization', ',', 'text', 'often', 'goes', 'through', 'stemming', ',', 'which', 'involves', 'reducing', 'words', 'to', 'their', 'base', 'or', 'root', 'form', 'by', 'removing', 'suffixes', '.', 'For', 'example', ',', 'words', 'like', '“', 'playing', '”', ',', '“', 'played', '”', ',', 'and', '“', 'plays', '”', 'are', 'reduced', 'to', '“', 'play', '”', '.', 'However', ',', 'stemming', 'is', 'a', 'rule-based', 'and', 'sometimes', 'crude', 'process', ',', 'often', 'leading', 'to', 'non-real', 'words', 'like', '“', 'comput', '”', 'from', '“', 'computing', '”', '.', 'To', 'overcome', 'this', ',', 'lemmatization', 'is', 'used', ',', 'which', 'is', 'a', 'more', 'sophisticated', 'technique', 'that', 'transforms', 'words', 'to', 'their', 'dictionary', 'form', '(', 'lemma', ')', ',', 'taking', 'into', 'account', 'the', 'context', 'and', 'parts', 'of', 'speech', '.', 'For', 'example', ',', '“', 'running', '”', 'becomes', '“', 'run', '”', ',', 'and', '“', 'better', '”', 'becomes', '“', 'good', '”', '.', 'Lemmatization', 'provides', 'more', 'meaningful', 'results', 'compared', 'to', 'stemming', ',', 'although', 'it', 'is', 'computationally', 'more', 'intensive', '.', 'Together', ',', 'these', 'techniques', 'are', 'crucial', 'in', 'cleaning', 'and', 'normalizing', 'text', 'for', 'downstream', 'NLP', 'tasks', 'like', 'classification', ',', 'sentiment', 'analysis', ',', 'and', 'information', 'retrieval', '.']\n",
      "Filtered Words: ['Natural', 'Language', 'Processing', '(', 'NLP', ')', ',', 'tokenization', 'foundational', 'step', 'raw', 'text', 'split', 'smaller', 'units', 'called', 'tokens', '.', 'tokens', 'words', ',', 'subwords', ',', 'even', 'characters', ',', 'depending', 'application', '.', 'Tokenization', 'helps', 'understanding', 'processing', 'text', 'breaking', 'manageable', 'pieces', '.', 'tokenization', ',', 'text', 'often', 'goes', 'stemming', ',', 'involves', 'reducing', 'words', 'base', 'root', 'form', 'removing', 'suffixes', '.', 'example', ',', 'words', 'like', '“', 'playing', '”', ',', '“', 'played', '”', ',', '“', 'plays', '”', 'reduced', '“', 'play', '”', '.', 'However', ',', 'stemming', 'rule-based', 'sometimes', 'crude', 'process', ',', 'often', 'leading', 'non-real', 'words', 'like', '“', 'comput', '”', '“', 'computing', '”', '.', 'overcome', ',', 'lemmatization', 'used', ',', 'sophisticated', 'technique', 'transforms', 'words', 'dictionary', 'form', '(', 'lemma', ')', ',', 'taking', 'account', 'context', 'parts', 'speech', '.', 'example', ',', '“', 'running', '”', 'becomes', '“', 'run', '”', ',', '“', 'better', '”', 'becomes', '“', 'good', '”', '.', 'Lemmatization', 'provides', 'meaningful', 'results', 'compared', 'stemming', ',', 'although', 'computationally', 'intensive', '.', 'Together', ',', 'techniques', 'crucial', 'cleaning', 'normalizing', 'text', 'downstream', 'NLP', 'tasks', 'like', 'classification', ',', 'sentiment', 'analysis', ',', 'information', 'retrieval', '.']\n",
      "Stemmed Words: ['natur', 'languag', 'process', '(', 'nlp', ')', ',', 'token', 'foundat', 'step', 'raw', 'text', 'split', 'smaller', 'unit', 'call', 'token', '.', 'token', 'word', ',', 'subword', ',', 'even', 'charact', ',', 'depend', 'applic', '.', 'token', 'help', 'understand', 'process', 'text', 'break', 'manag', 'piec', '.', 'token', ',', 'text', 'often', 'goe', 'stem', ',', 'involv', 'reduc', 'word', 'base', 'root', 'form', 'remov', 'suffix', '.', 'exampl', ',', 'word', 'like', '“', 'play', '”', ',', '“', 'play', '”', ',', '“', 'play', '”', 'reduc', '“', 'play', '”', '.', 'howev', ',', 'stem', 'rule-bas', 'sometim', 'crude', 'process', ',', 'often', 'lead', 'non-real', 'word', 'like', '“', 'comput', '”', '“', 'comput', '”', '.', 'overcom', ',', 'lemmat', 'use', ',', 'sophist', 'techniqu', 'transform', 'word', 'dictionari', 'form', '(', 'lemma', ')', ',', 'take', 'account', 'context', 'part', 'speech', '.', 'exampl', ',', '“', 'run', '”', 'becom', '“', 'run', '”', ',', '“', 'better', '”', 'becom', '“', 'good', '”', '.', 'lemmat', 'provid', 'meaning', 'result', 'compar', 'stem', ',', 'although', 'comput', 'intens', '.', 'togeth', ',', 'techniqu', 'crucial', 'clean', 'normal', 'text', 'downstream', 'nlp', 'task', 'like', 'classif', ',', 'sentiment', 'analysi', ',', 'inform', 'retriev', '.']\n",
      "Lemmatized Words: ['Natural', 'Language', 'Processing', '(', 'NLP', ')', ',', 'tokenization', 'foundational', 'step', 'raw', 'text', 'split', 'smaller', 'unit', 'called', 'token', '.', 'token', 'word', ',', 'subwords', ',', 'even', 'character', ',', 'depending', 'application', '.', 'Tokenization', 'help', 'understanding', 'processing', 'text', 'breaking', 'manageable', 'piece', '.', 'tokenization', ',', 'text', 'often', 'go', 'stemming', ',', 'involves', 'reducing', 'word', 'base', 'root', 'form', 'removing', 'suffix', '.', 'example', ',', 'word', 'like', '“', 'playing', '”', ',', '“', 'played', '”', ',', '“', 'play', '”', 'reduced', '“', 'play', '”', '.', 'However', ',', 'stemming', 'rule-based', 'sometimes', 'crude', 'process', ',', 'often', 'leading', 'non-real', 'word', 'like', '“', 'comput', '”', '“', 'computing', '”', '.', 'overcome', ',', 'lemmatization', 'used', ',', 'sophisticated', 'technique', 'transforms', 'word', 'dictionary', 'form', '(', 'lemma', ')', ',', 'taking', 'account', 'context', 'part', 'speech', '.', 'example', ',', '“', 'running', '”', 'becomes', '“', 'run', '”', ',', '“', 'better', '”', 'becomes', '“', 'good', '”', '.', 'Lemmatization', 'provides', 'meaningful', 'result', 'compared', 'stemming', ',', 'although', 'computationally', 'intensive', '.', 'Together', ',', 'technique', 'crucial', 'cleaning', 'normalizing', 'text', 'downstream', 'NLP', 'task', 'like', 'classification', ',', 'sentiment', 'analysis', ',', 'information', 'retrieval', '.']\n"
     ]
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "words = nltk.word_tokenize(paragraph)\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "print(\"Sentences:\", sentences)\n",
    "print(\"Words:\", words)\n",
    "print(\"Filtered Words:\", filtered_words)\n",
    "print(\"Stemmed Words:\", stemmed_words)\n",
    "print(\"Lemmatized Words:\", lemmatized_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab45c777",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agenticaienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
