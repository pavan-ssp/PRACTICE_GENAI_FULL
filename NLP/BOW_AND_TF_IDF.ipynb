{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc07ef0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5d0007c",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph =\"\"\" In Natural Language Processing (NLP), \n",
    "               tokenization is the foundational step where raw text is split into smaller units called tokens. \n",
    "               These tokens can be words, subwords, or even characters, depending on the application. \n",
    "               Tokenization helps in understanding and processing text by breaking it down into manageable pieces.\n",
    "               After tokenization, text often goes through stemming,\n",
    "               which involves reducing words to their base or root form by removing suffixes. \n",
    "               For example, words like “playing”, “played”, and “plays” are reduced to “play”. \n",
    "               However, stemming is a rule-based and sometimes crude process, often leading to non-real words like “comput” from “computing”.\n",
    "               To overcome this, lemmatization is used, which is a more sophisticated technique \n",
    "               that transforms words to their dictionary form (lemma), taking into account the context and parts of speech. \n",
    "               For example, “running” becomes “run”, and “better” becomes “good”. \n",
    "               Lemmatization provides more meaningful results compared to stemming, although it is computationally more intensive. \n",
    "               Together, these techniques are crucial in cleaning and normalizing text for downstream NLP tasks \n",
    "               like classification, sentiment analysis, and information retrieval.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff91d51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the texts\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8824b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "wordnet=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd86cfef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " '(',\n",
       " 'NLP',\n",
       " ')',\n",
       " ',',\n",
       " 'tokenization',\n",
       " 'is',\n",
       " 'the',\n",
       " 'foundational',\n",
       " 'step',\n",
       " 'where',\n",
       " 'raw',\n",
       " 'text',\n",
       " 'is',\n",
       " 'split',\n",
       " 'into',\n",
       " 'smaller',\n",
       " 'units',\n",
       " 'called',\n",
       " 'tokens',\n",
       " '.',\n",
       " 'These',\n",
       " 'tokens',\n",
       " 'can',\n",
       " 'be',\n",
       " 'words',\n",
       " ',',\n",
       " 'subwords',\n",
       " ',',\n",
       " 'or',\n",
       " 'even',\n",
       " 'characters',\n",
       " ',',\n",
       " 'depending',\n",
       " 'on',\n",
       " 'the',\n",
       " 'application',\n",
       " '.',\n",
       " 'Tokenization',\n",
       " 'helps',\n",
       " 'in',\n",
       " 'understanding',\n",
       " 'and',\n",
       " 'processing',\n",
       " 'text',\n",
       " 'by',\n",
       " 'breaking',\n",
       " 'it',\n",
       " 'down',\n",
       " 'into',\n",
       " 'manageable',\n",
       " 'pieces',\n",
       " '.',\n",
       " 'After',\n",
       " 'tokenization',\n",
       " ',',\n",
       " 'text',\n",
       " 'often',\n",
       " 'goes',\n",
       " 'through',\n",
       " 'stemming',\n",
       " ',',\n",
       " 'which',\n",
       " 'involves',\n",
       " 'reducing',\n",
       " 'words',\n",
       " 'to',\n",
       " 'their',\n",
       " 'base',\n",
       " 'or',\n",
       " 'root',\n",
       " 'form',\n",
       " 'by',\n",
       " 'removing',\n",
       " 'suffixes',\n",
       " '.',\n",
       " 'For',\n",
       " 'example',\n",
       " ',',\n",
       " 'words',\n",
       " 'like',\n",
       " '“',\n",
       " 'playing',\n",
       " '”',\n",
       " ',',\n",
       " '“',\n",
       " 'played',\n",
       " '”',\n",
       " ',',\n",
       " 'and',\n",
       " '“',\n",
       " 'plays',\n",
       " '”',\n",
       " 'are',\n",
       " 'reduced',\n",
       " 'to',\n",
       " '“',\n",
       " 'play',\n",
       " '”',\n",
       " '.',\n",
       " 'However',\n",
       " ',',\n",
       " 'stemming',\n",
       " 'is',\n",
       " 'a',\n",
       " 'rule-based',\n",
       " 'and',\n",
       " 'sometimes',\n",
       " 'crude',\n",
       " 'process',\n",
       " ',',\n",
       " 'often',\n",
       " 'leading',\n",
       " 'to',\n",
       " 'non-real',\n",
       " 'words',\n",
       " 'like',\n",
       " '“',\n",
       " 'comput',\n",
       " '”',\n",
       " 'from',\n",
       " '“',\n",
       " 'computing',\n",
       " '”',\n",
       " '.',\n",
       " 'To',\n",
       " 'overcome',\n",
       " 'this',\n",
       " ',',\n",
       " 'lemmatization',\n",
       " 'is',\n",
       " 'used',\n",
       " ',',\n",
       " 'which',\n",
       " 'is',\n",
       " 'a',\n",
       " 'more',\n",
       " 'sophisticated',\n",
       " 'technique',\n",
       " 'that',\n",
       " 'transforms',\n",
       " 'words',\n",
       " 'to',\n",
       " 'their',\n",
       " 'dictionary',\n",
       " 'form',\n",
       " '(',\n",
       " 'lemma',\n",
       " ')',\n",
       " ',',\n",
       " 'taking',\n",
       " 'into',\n",
       " 'account',\n",
       " 'the',\n",
       " 'context',\n",
       " 'and',\n",
       " 'parts',\n",
       " 'of',\n",
       " 'speech',\n",
       " '.',\n",
       " 'For',\n",
       " 'example',\n",
       " ',',\n",
       " '“',\n",
       " 'running',\n",
       " '”',\n",
       " 'becomes',\n",
       " '“',\n",
       " 'run',\n",
       " '”',\n",
       " ',',\n",
       " 'and',\n",
       " '“',\n",
       " 'better',\n",
       " '”',\n",
       " 'becomes',\n",
       " '“',\n",
       " 'good',\n",
       " '”',\n",
       " '.',\n",
       " 'Lemmatization',\n",
       " 'provides',\n",
       " 'more',\n",
       " 'meaningful',\n",
       " 'results',\n",
       " 'compared',\n",
       " 'to',\n",
       " 'stemming',\n",
       " ',',\n",
       " 'although',\n",
       " 'it',\n",
       " 'is',\n",
       " 'computationally',\n",
       " 'more',\n",
       " 'intensive',\n",
       " '.',\n",
       " 'Together',\n",
       " ',',\n",
       " 'these',\n",
       " 'techniques',\n",
       " 'are',\n",
       " 'crucial',\n",
       " 'in',\n",
       " 'cleaning',\n",
       " 'and',\n",
       " 'normalizing',\n",
       " 'text',\n",
       " 'for',\n",
       " 'downstream',\n",
       " 'NLP',\n",
       " 'tasks',\n",
       " 'like',\n",
       " 'classification',\n",
       " ',',\n",
       " 'sentiment',\n",
       " 'analysis',\n",
       " ',',\n",
       " 'and',\n",
       " 'information',\n",
       " 'retrieval',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "words = nltk.word_tokenize(paragraph)   \n",
    "sentences\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "658d3f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for i in range(len(sentences)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1459ca0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Bag of Words model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features = 1500)\n",
    "X_CV = cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9ffb7bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,\n",
       "        0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 4, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85460742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the TF_IDF Words model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "TF = TfidfVectorizer(max_features = 1500)\n",
    "X_TF = cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c041ff44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,\n",
       "        0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 4, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92401df5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agenticaienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
