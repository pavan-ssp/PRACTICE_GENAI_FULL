{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "380f30c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53670c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1ba1e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph =\"\"\" In Natural Language Processing (NLP), \n",
    "               tokenization is the foundational step where raw text is split into smaller units called tokens. \n",
    "               These tokens can be words, subwords, or even characters, depending on the application. \n",
    "               Tokenization helps in understanding and processing text by breaking it down into manageable pieces.\n",
    "               After tokenization, text often goes through stemming,\n",
    "               which involves reducing words to their base or root form by removing suffixes. \n",
    "               For example, words like “playing”, “played”, and “plays” are reduced to “play”. \n",
    "               However, stemming is a rule-based and sometimes crude process, often leading to non-real words like “comput” from “computing”.\n",
    "               To overcome this, lemmatization is used, which is a more sophisticated technique \n",
    "               that transforms words to their dictionary form (lemma), taking into account the context and parts of speech. \n",
    "               For example, “running” becomes “run”, and “better” becomes “good”. \n",
    "               Lemmatization provides more meaningful results compared to stemming, although it is computationally more intensive. \n",
    "               Together, these techniques are crucial in cleaning and normalizing text for downstream NLP tasks \n",
    "               like classification, sentiment analysis, and information retrieval.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a736331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' In Natural Language Processing (NLP), \\n               tokenization is the foundational step where raw text is split into smaller units called tokens.',\n",
       " 'These tokens can be words, subwords, or even characters, depending on the application.',\n",
       " 'Tokenization helps in understanding and processing text by breaking it down into manageable pieces.',\n",
       " 'After tokenization, text often goes through stemming,\\n               which involves reducing words to their base or root form by removing suffixes.',\n",
       " 'For example, words like “playing”, “played”, and “plays” are reduced to “play”.',\n",
       " 'However, stemming is a rule-based and sometimes crude process, often leading to non-real words like “comput” from “computing”.',\n",
       " 'To overcome this, lemmatization is used, which is a more sophisticated technique \\n               that transforms words to their dictionary form (lemma), taking into account the context and parts of speech.',\n",
       " 'For example, “running” becomes “run”, and “better” becomes “good”.',\n",
       " 'Lemmatization provides more meaningful results compared to stemming, although it is computationally more intensive.',\n",
       " 'Together, these techniques are crucial in cleaning and normalizing text for downstream NLP tasks \\n               like classification, sentiment analysis, and information retrieval.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences= nltk.sent_tokenize(paragraph)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a927485f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " '(',\n",
       " 'NLP',\n",
       " ')',\n",
       " ',',\n",
       " 'tokenization',\n",
       " 'is',\n",
       " 'the',\n",
       " 'foundational',\n",
       " 'step',\n",
       " 'where',\n",
       " 'raw',\n",
       " 'text',\n",
       " 'is',\n",
       " 'split',\n",
       " 'into',\n",
       " 'smaller',\n",
       " 'units',\n",
       " 'called',\n",
       " 'tokens',\n",
       " '.',\n",
       " 'These',\n",
       " 'tokens',\n",
       " 'can',\n",
       " 'be',\n",
       " 'words',\n",
       " ',',\n",
       " 'subwords',\n",
       " ',',\n",
       " 'or',\n",
       " 'even',\n",
       " 'characters',\n",
       " ',',\n",
       " 'depending',\n",
       " 'on',\n",
       " 'the',\n",
       " 'application',\n",
       " '.',\n",
       " 'Tokenization',\n",
       " 'helps',\n",
       " 'in',\n",
       " 'understanding',\n",
       " 'and',\n",
       " 'processing',\n",
       " 'text',\n",
       " 'by',\n",
       " 'breaking',\n",
       " 'it',\n",
       " 'down',\n",
       " 'into',\n",
       " 'manageable',\n",
       " 'pieces',\n",
       " '.',\n",
       " 'After',\n",
       " 'tokenization',\n",
       " ',',\n",
       " 'text',\n",
       " 'often',\n",
       " 'goes',\n",
       " 'through',\n",
       " 'stemming',\n",
       " ',',\n",
       " 'which',\n",
       " 'involves',\n",
       " 'reducing',\n",
       " 'words',\n",
       " 'to',\n",
       " 'their',\n",
       " 'base',\n",
       " 'or',\n",
       " 'root',\n",
       " 'form',\n",
       " 'by',\n",
       " 'removing',\n",
       " 'suffixes',\n",
       " '.',\n",
       " 'For',\n",
       " 'example',\n",
       " ',',\n",
       " 'words',\n",
       " 'like',\n",
       " '“',\n",
       " 'playing',\n",
       " '”',\n",
       " ',',\n",
       " '“',\n",
       " 'played',\n",
       " '”',\n",
       " ',',\n",
       " 'and',\n",
       " '“',\n",
       " 'plays',\n",
       " '”',\n",
       " 'are',\n",
       " 'reduced',\n",
       " 'to',\n",
       " '“',\n",
       " 'play',\n",
       " '”',\n",
       " '.',\n",
       " 'However',\n",
       " ',',\n",
       " 'stemming',\n",
       " 'is',\n",
       " 'a',\n",
       " 'rule-based',\n",
       " 'and',\n",
       " 'sometimes',\n",
       " 'crude',\n",
       " 'process',\n",
       " ',',\n",
       " 'often',\n",
       " 'leading',\n",
       " 'to',\n",
       " 'non-real',\n",
       " 'words',\n",
       " 'like',\n",
       " '“',\n",
       " 'comput',\n",
       " '”',\n",
       " 'from',\n",
       " '“',\n",
       " 'computing',\n",
       " '”',\n",
       " '.',\n",
       " 'To',\n",
       " 'overcome',\n",
       " 'this',\n",
       " ',',\n",
       " 'lemmatization',\n",
       " 'is',\n",
       " 'used',\n",
       " ',',\n",
       " 'which',\n",
       " 'is',\n",
       " 'a',\n",
       " 'more',\n",
       " 'sophisticated',\n",
       " 'technique',\n",
       " 'that',\n",
       " 'transforms',\n",
       " 'words',\n",
       " 'to',\n",
       " 'their',\n",
       " 'dictionary',\n",
       " 'form',\n",
       " '(',\n",
       " 'lemma',\n",
       " ')',\n",
       " ',',\n",
       " 'taking',\n",
       " 'into',\n",
       " 'account',\n",
       " 'the',\n",
       " 'context',\n",
       " 'and',\n",
       " 'parts',\n",
       " 'of',\n",
       " 'speech',\n",
       " '.',\n",
       " 'For',\n",
       " 'example',\n",
       " ',',\n",
       " '“',\n",
       " 'running',\n",
       " '”',\n",
       " 'becomes',\n",
       " '“',\n",
       " 'run',\n",
       " '”',\n",
       " ',',\n",
       " 'and',\n",
       " '“',\n",
       " 'better',\n",
       " '”',\n",
       " 'becomes',\n",
       " '“',\n",
       " 'good',\n",
       " '”',\n",
       " '.',\n",
       " 'Lemmatization',\n",
       " 'provides',\n",
       " 'more',\n",
       " 'meaningful',\n",
       " 'results',\n",
       " 'compared',\n",
       " 'to',\n",
       " 'stemming',\n",
       " ',',\n",
       " 'although',\n",
       " 'it',\n",
       " 'is',\n",
       " 'computationally',\n",
       " 'more',\n",
       " 'intensive',\n",
       " '.',\n",
       " 'Together',\n",
       " ',',\n",
       " 'these',\n",
       " 'techniques',\n",
       " 'are',\n",
       " 'crucial',\n",
       " 'in',\n",
       " 'cleaning',\n",
       " 'and',\n",
       " 'normalizing',\n",
       " 'text',\n",
       " 'for',\n",
       " 'downstream',\n",
       " 'NLP',\n",
       " 'tasks',\n",
       " 'like',\n",
       " 'classification',\n",
       " ',',\n",
       " 'sentiment',\n",
       " 'analysis',\n",
       " ',',\n",
       " 'and',\n",
       " 'information',\n",
       " 'retrieval',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words= nltk.word_tokenize(paragraph)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6626f481",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agenticaienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
